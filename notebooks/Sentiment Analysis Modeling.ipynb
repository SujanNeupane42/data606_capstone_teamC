{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cG8_xuiA0ppo","executionInfo":{"status":"ok","timestamp":1755006275798,"user_tz":240,"elapsed":28705,"user":{"displayName":"Danny McKirgan","userId":"02989772314703733078"}},"outputId":"4c66fb03-4256-4747-d657-e87f048f1683"},"id":"cG8_xuiA0ppo","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8597995","executionInfo":{"status":"ok","timestamp":1755008502830,"user_tz":240,"elapsed":2117055,"user":{"displayName":"Danny McKirgan","userId":"02989772314703733078"}},"outputId":"d876457e-37c8-4d23-dcf2-11a5f6920a9f"},"source":["import nltk\n","nltk.download('vader_lexicon')\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report\n","import pandas as pd\n","import textstat\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from sklearn.preprocessing import LabelEncoder\n","from imblearn.over_sampling import RandomOverSampler\n","\n","# Load the data\n","df=pd.read_csv('/content/gdrive/MyDrive/DATA 606 Coding Files/news_sentiment_analysis.csv')\n","\n","# Feature Engineering - VADAR and Reading Scores\n","df['text'] = df['Title'].fillna('') + ' ' + df['Description'].fillna('')\n","sid = SentimentIntensityAnalyzer()\n","vader_scores = df['text'].apply(lambda x: sid.polarity_scores(x))\n","df = pd.concat([df, vader_scores.apply(pd.Series)], axis=1)\n","df['flesch_kincaid_grade'] = df['text'].apply(textstat.flesch_kincaid_grade)\n","\n","print(df.head())\n","df.describe()\n","\n","# Labeling, SMOTE, and Under Sampling\n","le = LabelEncoder()\n","df['sentiment_enc'] = le.fit_transform(df['Sentiment'])\n","target = df[df['Sentiment']=='neutral'].shape[0]\n","\n","# Prepare for sampling\n","X = df[['text','neg','neu','pos','compound','flesch_kincaid_grade']]\n","y = df['sentiment_enc'] # Use the encoded sentiment\n","\n","# Oversample only the negative class up to neutral\n","ros = RandomOverSampler(sampling_strategy={'negative': target}, random_state=42)\n","# Note: ros.fit_resample expects numerical labels, so we use y which is sentiment_enc\n","X_ros, y_ros = ros.fit_resample(X, df['Sentiment']) # Resample using original sentiment for clarity, then re-encode\n","\n","# Re-encode after resampling to ensure correct numerical labels for the models\n","y_ros_enc = le.transform(y_ros)\n","\n","# Split data after resampling\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_ros, y_ros_enc, test_size=0.2, random_state=42, stratify=y_ros_enc # Stratify to maintain class distribution\n",")\n","\n","\n","# Separate text and numerical features and apply TfidfVectorizer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('text_features', TfidfVectorizer(), 'text')\n","    ],\n","    remainder='passthrough'\n",")\n","\n","# Define models and parameter grids\n","models = {\n","    'Logistic Regression': LogisticRegression(max_iter=1000),\n","    'XGBoost'            : XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n","    'MLP Classifier'     : MLPClassifier(max_iter=300)\n","}\n","\n","param_grids = {\n","    'Logistic Regression': {\n","        'clf__C':      [20],\n","        'clf__penalty':['l2'],\n","        'clf__solver': ['lbfgs']\n","    },\n","    'XGBoost': {\n","        'clf__n_estimators': [100, 150],\n","        'clf__max_depth':    [15, 20, 30],\n","        'clf__learning_rate':[0.1]\n","    },\n","    'MLP Classifier': {\n","        'clf__hidden_layer_sizes': [(100,), (150,)],\n","        'clf__alpha':              [0.001, 0.01,0.1],\n","        'clf__learning_rate_init': [0.01, 0.1]\n","    }\n","}\n","\n","# Grid search & evaluation\n","for name, estimator in models.items():\n","    pipe = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('clf',   estimator)\n","    ])\n","    grid = GridSearchCV(\n","        pipe,\n","        param_grids[name],\n","        cv=3,\n","        scoring='f1_macro',\n","        n_jobs=-1,\n","        verbose=1\n","    )\n","    grid.fit(X_train, y_train)\n","\n","    best = grid.best_estimator_\n","    print(f\"\\n{name}\")\n","    print(\"Best hyperparameters:\", grid.best_params_)\n","\n","    # Train report\n","    y_train_pred = best.predict(X_train)\n","    print(\"\\nTrain set report:\")\n","    print(classification_report(y_train, y_train_pred, target_names=le.classes_))\n","\n","    # Test report\n","    y_test_pred = best.predict(X_test)\n","    print(\"Test set report:\")\n","    print(classification_report(y_test, y_test_pred, target_names=le.classes_))"],"id":"f8597995","execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["          Source          Author  \\\n","0        stgnews  Bridger Palmer   \n","1  Zimbabwe Mail  Staff Reporter   \n","2      4-traders             NaN   \n","3      4-traders             NaN   \n","4         PLANET             NaN   \n","\n","                                               Title  \\\n","0  Pine View High teacher wins Best in State awar...   \n","1  Businesses Face Financial Strain Amid Liquidit...   \n","2  Musk donates to super pac working to elect Tru...   \n","3  US FTC issues warning to franchisors over unfa...   \n","4                          Rooftop solar's dark side   \n","\n","                                         Description  \\\n","0  ST. GEORGE — Kaitlyn Larson, a first-year teac...   \n","1  Harare, Zimbabwe – Local businesses are grappl...   \n","2  (marketscreener.com) Billionaire Elon Musk has...   \n","3  (marketscreener.com) A U.S. trade regulator on...   \n","4  4.5 million households in the U.S. have solar ...   \n","\n","                                                 URL  \\\n","0  https://www.stgeorgeutah.com/news/archive/2024...   \n","1  https://www.thezimbabwemail.com/business/busin...   \n","2  https://www.marketscreener.com/business-leader...   \n","3  https://www.marketscreener.com/quote/stock/MCD...   \n","4  https://www.npr.org/2024/07/12/1197961036/roof...   \n","\n","                Published At Sentiment      Type  \\\n","0  2024-07-12T23:45:25+00:00  positive  Business   \n","1  2024-07-12T22:59:42+00:00   neutral  Business   \n","2  2024-07-12T22:52:55+00:00  positive  Business   \n","3  2024-07-12T22:41:01+00:00  negative  Business   \n","4  2024-07-12T22:28:19+00:00  positive  Business   \n","\n","                                                text    neg    neu    pos  \\\n","0  Pine View High teacher wins Best in State awar...  0.000  0.762  0.238   \n","1  Businesses Face Financial Strain Amid Liquidit...  0.167  0.833  0.000   \n","2  Musk donates to super pac working to elect Tru...  0.000  0.918  0.082   \n","3  US FTC issues warning to franchisors over unfa...  0.160  0.840  0.000   \n","4  Rooftop solar's dark side 4.5 million househol...  0.122  0.837  0.041   \n","\n","   compound  flesch_kincaid_grade  \n","0    0.9643              8.045882  \n","1   -0.4215             16.761905  \n","2    0.5994             15.562500  \n","3   -0.8360             17.530517  \n","4   -0.8654              5.919564  \n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","\n","Logistic Regression\n","Best hyperparameters: {'clf__C': 20, 'clf__penalty': 'l2', 'clf__solver': 'lbfgs'}\n","\n","Train set report:\n","              precision    recall  f1-score   support\n","\n","    negative       1.00      1.00      1.00       631\n","     neutral       1.00      1.00      1.00       631\n","    positive       1.00      1.00      1.00      1707\n","\n","    accuracy                           1.00      2969\n","   macro avg       1.00      1.00      1.00      2969\n","weighted avg       1.00      1.00      1.00      2969\n","\n","Test set report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.91      1.00      0.95       158\n","     neutral       0.96      0.78      0.86       158\n","    positive       0.93      0.96      0.95       427\n","\n","    accuracy                           0.93       743\n","   macro avg       0.93      0.91      0.92       743\n","weighted avg       0.93      0.93      0.93       743\n","\n","Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [14:06:58] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["\n","XGBoost\n","Best hyperparameters: {'clf__learning_rate': 0.1, 'clf__max_depth': 30, 'clf__n_estimators': 100}\n","\n","Train set report:\n","              precision    recall  f1-score   support\n","\n","    negative       1.00      1.00      1.00       631\n","     neutral       1.00      1.00      1.00       631\n","    positive       1.00      1.00      1.00      1707\n","\n","    accuracy                           1.00      2969\n","   macro avg       1.00      1.00      1.00      2969\n","weighted avg       1.00      1.00      1.00      2969\n","\n","Test set report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.96      0.99      0.97       158\n","     neutral       0.89      0.82      0.86       158\n","    positive       0.94      0.95      0.94       427\n","\n","    accuracy                           0.93       743\n","   macro avg       0.93      0.92      0.92       743\n","weighted avg       0.93      0.93      0.93       743\n","\n","Fitting 3 folds for each of 12 candidates, totalling 36 fits\n","\n","MLP Classifier\n","Best hyperparameters: {'clf__alpha': 0.1, 'clf__hidden_layer_sizes': (150,), 'clf__learning_rate_init': 0.01}\n","\n","Train set report:\n","              precision    recall  f1-score   support\n","\n","    negative       1.00      0.93      0.96       631\n","     neutral       0.98      0.99      0.99       631\n","    positive       0.97      1.00      0.99      1707\n","\n","    accuracy                           0.98      2969\n","   macro avg       0.99      0.97      0.98      2969\n","weighted avg       0.98      0.98      0.98      2969\n","\n","Test set report:\n","              precision    recall  f1-score   support\n","\n","    negative       0.94      0.93      0.94       158\n","     neutral       0.93      0.79      0.86       158\n","    positive       0.91      0.96      0.93       427\n","\n","    accuracy                           0.92       743\n","   macro avg       0.93      0.89      0.91       743\n","weighted avg       0.92      0.92      0.92       743\n","\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}